{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from gym.envs.registration import register\n",
    "# register(\n",
    "#    id='FrozenLakeNotSlippery-v0',\n",
    "#    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=0.78, # optimum = .8196\n",
    "# )\n",
    "\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 0 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n",
      "Episode 1000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained: 0. Times reached goal: 92.\n",
      "Episode 2000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained: 0. Times reached goal: 105.\n",
      "Episode 3000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained: 0. Times reached goal: 110.\n",
      "Episode 4000 finished after 0 timesteps with r=1.0. Running score: 0.13. Times trained: 0. Times reached goal: 120.\n",
      "Episode 5000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained: 0. Times reached goal: 139.\n",
      "Episode 6000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained: 0. Times reached goal: 117.\n",
      "Episode 7000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained: 0. Times reached goal: 149.\n",
      "Episode 8000 finished after 0 timesteps with r=1.0. Running score: 0.16. Times trained: 0. Times reached goal: 144.\n",
      "Episode 9000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained: 0. Times reached goal: 129.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 10000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained: 0. Times reached goal: 153.\n",
      "Episode 11000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained: 0. Times reached goal: 199.\n",
      "Episode 12000 finished after 0 timesteps with r=0.0. Running score: 0.26. Times trained: 0. Times reached goal: 163.\n",
      "Episode 13000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained: 0. Times reached goal: 179.\n",
      "Episode 14000 finished after 0 timesteps with r=1.0. Running score: 0.24. Times trained: 0. Times reached goal: 194.\n",
      "Episode 15000 finished after 0 timesteps with r=1.0. Running score: 0.23. Times trained: 0. Times reached goal: 192.\n",
      "Episode 16000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained: 0. Times reached goal: 203.\n",
      "Episode 17000 finished after 0 timesteps with r=1.0. Running score: 0.19. Times trained: 0. Times reached goal: 201.\n",
      "Episode 18000 finished after 0 timesteps with r=0.0. Running score: 0.29. Times trained: 0. Times reached goal: 234.\n",
      "Episode 19000 finished after 0 timesteps with r=1.0. Running score: 0.23. Times trained: 0. Times reached goal: 198.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "[0.531441 0.       0.6561   0.      ]\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "q_values \n",
      "[0.59049 0.729   0.59049 0.     ]\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "[0.531441 0.       0.6561   0.      ]\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 20000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained: 0. Times reached goal: 237.\n",
      "Episode 21000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained: 0. Times reached goal: 203.\n",
      "Episode 22000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained: 0. Times reached goal: 230.\n",
      "Episode 23000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained: 0. Times reached goal: 269.\n",
      "Episode 24000 finished after 0 timesteps with r=0.0. Running score: 0.29. Times trained: 0. Times reached goal: 248.\n",
      "Episode 25000 finished after 0 timesteps with r=1.0. Running score: 0.26. Times trained: 0. Times reached goal: 251.\n",
      "Episode 26000 finished after 0 timesteps with r=0.0. Running score: 0.34. Times trained: 0. Times reached goal: 267.\n",
      "Episode 27000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained: 0. Times reached goal: 245.\n",
      "Episode 28000 finished after 0 timesteps with r=0.0. Running score: 0.29. Times trained: 0. Times reached goal: 283.\n",
      "Episode 29000 finished after 0 timesteps with r=0.0. Running score: 0.26. Times trained: 0. Times reached goal: 284.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 30000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained: 0. Times reached goal: 304.\n",
      "Episode 31000 finished after 0 timesteps with r=0.0. Running score: 0.31. Times trained: 0. Times reached goal: 282.\n",
      "Episode 32000 finished after 0 timesteps with r=1.0. Running score: 0.37. Times trained: 0. Times reached goal: 347.\n",
      "Episode 33000 finished after 0 timesteps with r=1.0. Running score: 0.28. Times trained: 0. Times reached goal: 311.\n",
      "Episode 34000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained: 0. Times reached goal: 341.\n",
      "Episode 35000 finished after 0 timesteps with r=0.0. Running score: 0.3. Times trained: 0. Times reached goal: 334.\n",
      "Episode 36000 finished after 0 timesteps with r=0.0. Running score: 0.38. Times trained: 0. Times reached goal: 351.\n",
      "Episode 37000 finished after 0 timesteps with r=1.0. Running score: 0.42. Times trained: 0. Times reached goal: 354.\n",
      "Episode 38000 finished after 0 timesteps with r=0.0. Running score: 0.34. Times trained: 0. Times reached goal: 345.\n",
      "Episode 39000 finished after 0 timesteps with r=1.0. Running score: 0.31. Times trained: 0. Times reached goal: 376.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=0 , \n",
      "new state=12, done=True\n",
      "Episode 40000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained: 0. Times reached goal: 362.\n",
      "Episode 41000 finished after 0 timesteps with r=1.0. Running score: 0.32. Times trained: 0. Times reached goal: 380.\n",
      "Episode 42000 finished after 0 timesteps with r=0.0. Running score: 0.35. Times trained: 0. Times reached goal: 372.\n",
      "Episode 43000 finished after 0 timesteps with r=1.0. Running score: 0.46. Times trained: 0. Times reached goal: 387.\n",
      "Episode 44000 finished after 0 timesteps with r=1.0. Running score: 0.39. Times trained: 0. Times reached goal: 422.\n",
      "Episode 45000 finished after 0 timesteps with r=1.0. Running score: 0.39. Times trained: 0. Times reached goal: 417.\n",
      "Episode 46000 finished after 0 timesteps with r=0.0. Running score: 0.37. Times trained: 0. Times reached goal: 430.\n",
      "Episode 47000 finished after 0 timesteps with r=0.0. Running score: 0.41. Times trained: 0. Times reached goal: 409.\n",
      "Episode 48000 finished after 0 timesteps with r=0.0. Running score: 0.52. Times trained: 0. Times reached goal: 433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49000 finished after 0 timesteps with r=0.0. Running score: 0.45. Times trained: 0. Times reached goal: 403.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 50000 finished after 0 timesteps with r=1.0. Running score: 0.45. Times trained: 0. Times reached goal: 423.\n",
      "Episode 51000 finished after 0 timesteps with r=1.0. Running score: 0.34. Times trained: 0. Times reached goal: 452.\n",
      "Episode 52000 finished after 0 timesteps with r=1.0. Running score: 0.45. Times trained: 0. Times reached goal: 469.\n",
      "Episode 53000 finished after 0 timesteps with r=1.0. Running score: 0.4. Times trained: 0. Times reached goal: 451.\n",
      "Episode 54000 finished after 0 timesteps with r=1.0. Running score: 0.44. Times trained: 0. Times reached goal: 457.\n",
      "Episode 55000 finished after 0 timesteps with r=1.0. Running score: 0.51. Times trained: 0. Times reached goal: 482.\n",
      "Episode 56000 finished after 0 timesteps with r=0.0. Running score: 0.43. Times trained: 0. Times reached goal: 479.\n",
      "Episode 57000 finished after 0 timesteps with r=0.0. Running score: 0.45. Times trained: 0. Times reached goal: 475.\n",
      "Episode 58000 finished after 0 timesteps with r=0.0. Running score: 0.47. Times trained: 0. Times reached goal: 460.\n",
      "Episode 59000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 492.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "[0.531441 0.       0.6561   0.      ]\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "q_values \n",
      "[0.59049 0.729   0.59049 0.     ]\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "q_values \n",
      "[0.   0.81 0.   0.  ]\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 60000 finished after 0 timesteps with r=0.0. Running score: 0.53. Times trained: 0. Times reached goal: 497.\n",
      "Episode 61000 finished after 0 timesteps with r=0.0. Running score: 0.44. Times trained: 0. Times reached goal: 498.\n",
      "Episode 62000 finished after 0 timesteps with r=0.0. Running score: 0.52. Times trained: 0. Times reached goal: 509.\n",
      "Episode 63000 finished after 0 timesteps with r=1.0. Running score: 0.48. Times trained: 0. Times reached goal: 519.\n",
      "Episode 64000 finished after 0 timesteps with r=1.0. Running score: 0.47. Times trained: 0. Times reached goal: 507.\n",
      "Episode 65000 finished after 0 timesteps with r=1.0. Running score: 0.49. Times trained: 0. Times reached goal: 538.\n",
      "Episode 66000 finished after 0 timesteps with r=0.0. Running score: 0.58. Times trained: 0. Times reached goal: 542.\n",
      "Episode 67000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 538.\n",
      "Episode 68000 finished after 0 timesteps with r=1.0. Running score: 0.63. Times trained: 0. Times reached goal: 565.\n",
      "Episode 69000 finished after 0 timesteps with r=0.0. Running score: 0.53. Times trained: 0. Times reached goal: 534.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 70000 finished after 0 timesteps with r=1.0. Running score: 0.51. Times trained: 0. Times reached goal: 565.\n",
      "Episode 71000 finished after 0 timesteps with r=0.0. Running score: 0.65. Times trained: 0. Times reached goal: 596.\n",
      "Episode 72000 finished after 0 timesteps with r=1.0. Running score: 0.6. Times trained: 0. Times reached goal: 570.\n",
      "Episode 73000 finished after 0 timesteps with r=1.0. Running score: 0.64. Times trained: 0. Times reached goal: 585.\n",
      "Episode 74000 finished after 0 timesteps with r=0.0. Running score: 0.55. Times trained: 0. Times reached goal: 590.\n",
      "Episode 75000 finished after 0 timesteps with r=1.0. Running score: 0.6. Times trained: 0. Times reached goal: 587.\n",
      "Episode 76000 finished after 0 timesteps with r=0.0. Running score: 0.6. Times trained: 0. Times reached goal: 595.\n",
      "Episode 77000 finished after 0 timesteps with r=0.0. Running score: 0.62. Times trained: 0. Times reached goal: 602.\n",
      "Episode 78000 finished after 0 timesteps with r=1.0. Running score: 0.62. Times trained: 0. Times reached goal: 620.\n",
      "Episode 79000 finished after 0 timesteps with r=1.0. Running score: 0.67. Times trained: 0. Times reached goal: 622.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 80000 finished after 0 timesteps with r=1.0. Running score: 0.61. Times trained: 0. Times reached goal: 596.\n",
      "Episode 81000 finished after 0 timesteps with r=1.0. Running score: 0.62. Times trained: 0. Times reached goal: 603.\n",
      "Episode 82000 finished after 0 timesteps with r=1.0. Running score: 0.7. Times trained: 0. Times reached goal: 654.\n",
      "Episode 83000 finished after 0 timesteps with r=1.0. Running score: 0.63. Times trained: 0. Times reached goal: 639.\n",
      "Episode 84000 finished after 0 timesteps with r=0.0. Running score: 0.57. Times trained: 0. Times reached goal: 671.\n",
      "Episode 85000 finished after 0 timesteps with r=1.0. Running score: 0.68. Times trained: 0. Times reached goal: 627.\n",
      "Episode 86000 finished after 0 timesteps with r=0.0. Running score: 0.63. Times trained: 0. Times reached goal: 633.\n",
      "Episode 87000 finished after 0 timesteps with r=0.0. Running score: 0.67. Times trained: 0. Times reached goal: 646.\n",
      "Episode 88000 finished after 0 timesteps with r=0.0. Running score: 0.56. Times trained: 0. Times reached goal: 669.\n",
      "Episode 89000 finished after 0 timesteps with r=0.0. Running score: 0.71. Times trained: 0. Times reached goal: 642.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 90000 finished after 0 timesteps with r=1.0. Running score: 0.6. Times trained: 0. Times reached goal: 625.\n",
      "Episode 91000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 637.\n",
      "Episode 92000 finished after 0 timesteps with r=0.0. Running score: 0.61. Times trained: 0. Times reached goal: 654.\n",
      "Episode 93000 finished after 0 timesteps with r=1.0. Running score: 0.67. Times trained: 0. Times reached goal: 660.\n",
      "Episode 94000 finished after 0 timesteps with r=0.0. Running score: 0.72. Times trained: 0. Times reached goal: 671.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 95000 finished after 0 timesteps with r=0.0. Running score: 0.73. Times trained: 0. Times reached goal: 679.\n",
      "Episode 96000 finished after 0 timesteps with r=1.0. Running score: 0.66. Times trained: 0. Times reached goal: 682.\n",
      "Episode 97000 finished after 0 timesteps with r=1.0. Running score: 0.65. Times trained: 0. Times reached goal: 669.\n",
      "Episode 98000 finished after 0 timesteps with r=1.0. Running score: 0.69. Times trained: 0. Times reached goal: 697.\n",
      "Episode 99000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 714.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=2 , \n",
      "new state=10, done=False\n",
      "q_values \n",
      "[0.729 0.9   0.    0.   ]\n",
      "On state=10, selected action=1 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 100000 finished after 0 timesteps with r=1.0. Running score: 0.68. Times trained: 0. Times reached goal: 712.\n",
      "Episode 101000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained: 0. Times reached goal: 709.\n",
      "Episode 102000 finished after 0 timesteps with r=1.0. Running score: 0.74. Times trained: 0. Times reached goal: 716.\n",
      "Episode 103000 finished after 0 timesteps with r=0.0. Running score: 0.73. Times trained: 0. Times reached goal: 706.\n",
      "Episode 104000 finished after 0 timesteps with r=0.0. Running score: 0.72. Times trained: 0. Times reached goal: 725.\n",
      "Episode 105000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 725.\n",
      "Episode 106000 finished after 0 timesteps with r=1.0. Running score: 0.76. Times trained: 0. Times reached goal: 725.\n",
      "Episode 107000 finished after 0 timesteps with r=0.0. Running score: 0.69. Times trained: 0. Times reached goal: 715.\n",
      "Episode 108000 finished after 0 timesteps with r=1.0. Running score: 0.65. Times trained: 0. Times reached goal: 705.\n",
      "Episode 109000 finished after 0 timesteps with r=1.0. Running score: 0.71. Times trained: 0. Times reached goal: 728.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 110000 finished after 0 timesteps with r=1.0. Running score: 0.73. Times trained: 0. Times reached goal: 723.\n",
      "Episode 111000 finished after 0 timesteps with r=0.0. Running score: 0.83. Times trained: 0. Times reached goal: 732.\n",
      "Episode 112000 finished after 0 timesteps with r=1.0. Running score: 0.71. Times trained: 0. Times reached goal: 734.\n",
      "Episode 113000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 748.\n",
      "Episode 114000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 760.\n",
      "Episode 115000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained: 0. Times reached goal: 761.\n",
      "Episode 116000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 748.\n",
      "Episode 117000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 762.\n",
      "Episode 118000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained: 0. Times reached goal: 756.\n",
      "Episode 119000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained: 0. Times reached goal: 755.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=1 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 120000 finished after 0 timesteps with r=1.0. Running score: 0.74. Times trained: 0. Times reached goal: 770.\n",
      "Episode 121000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained: 0. Times reached goal: 761.\n",
      "Episode 122000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 767.\n",
      "Episode 123000 finished after 0 timesteps with r=1.0. Running score: 0.7. Times trained: 0. Times reached goal: 760.\n",
      "Episode 124000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained: 0. Times reached goal: 785.\n",
      "Episode 125000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained: 0. Times reached goal: 784.\n",
      "Episode 126000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained: 0. Times reached goal: 764.\n",
      "Episode 127000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 775.\n",
      "Episode 128000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 790.\n",
      "Episode 129000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 799.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 130000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 787.\n",
      "Episode 131000 finished after 0 timesteps with r=0.0. Running score: 0.77. Times trained: 0. Times reached goal: 807.\n",
      "Episode 132000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained: 0. Times reached goal: 789.\n",
      "Episode 133000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained: 0. Times reached goal: 808.\n",
      "Episode 134000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 820.\n",
      "Episode 135000 finished after 0 timesteps with r=0.0. Running score: 0.78. Times trained: 0. Times reached goal: 811.\n",
      "Episode 136000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 792.\n",
      "Episode 137000 finished after 0 timesteps with r=0.0. Running score: 0.86. Times trained: 0. Times reached goal: 809.\n",
      "Episode 138000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained: 0. Times reached goal: 798.\n",
      "Episode 139000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 825.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=2 , \n",
      "new state=10, done=False\n",
      "q_values \n",
      "[0.729 0.9   0.    0.   ]\n",
      "On state=10, selected action=1 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=0 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=1 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 140000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained: 0. Times reached goal: 809.\n",
      "Episode 141000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 807.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 142000 finished after 0 timesteps with r=0.0. Running score: 0.82. Times trained: 0. Times reached goal: 817.\n",
      "Episode 143000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained: 0. Times reached goal: 813.\n",
      "Episode 144000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 838.\n",
      "Episode 145000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained: 0. Times reached goal: 829.\n",
      "Episode 146000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 838.\n",
      "Episode 147000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 831.\n",
      "Episode 148000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained: 0. Times reached goal: 834.\n",
      "Episode 149000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 839.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 150000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 840.\n",
      "Episode 151000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 831.\n",
      "Episode 152000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 853.\n",
      "Episode 153000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained: 0. Times reached goal: 836.\n",
      "Episode 154000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 857.\n",
      "Episode 155000 finished after 0 timesteps with r=0.0. Running score: 0.84. Times trained: 0. Times reached goal: 865.\n",
      "Episode 156000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 836.\n",
      "Episode 157000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 848.\n",
      "Episode 158000 finished after 0 timesteps with r=0.0. Running score: 0.87. Times trained: 0. Times reached goal: 840.\n",
      "Episode 159000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 851.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 160000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained: 0. Times reached goal: 859.\n",
      "Episode 161000 finished after 0 timesteps with r=0.0. Running score: 0.84. Times trained: 0. Times reached goal: 867.\n",
      "Episode 162000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained: 0. Times reached goal: 848.\n",
      "Episode 163000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained: 0. Times reached goal: 862.\n",
      "Episode 164000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained: 0. Times reached goal: 858.\n",
      "Episode 165000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 866.\n",
      "Episode 166000 finished after 0 timesteps with r=0.0. Running score: 0.88. Times trained: 0. Times reached goal: 867.\n",
      "Episode 167000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 843.\n",
      "Episode 168000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 860.\n",
      "Episode 169000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 854.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 170000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained: 0. Times reached goal: 878.\n",
      "Episode 171000 finished after 0 timesteps with r=0.0. Running score: 0.87. Times trained: 0. Times reached goal: 865.\n",
      "Episode 172000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 875.\n",
      "Episode 173000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 865.\n",
      "Episode 174000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 866.\n",
      "Episode 175000 finished after 0 timesteps with r=0.0. Running score: 0.86. Times trained: 0. Times reached goal: 886.\n",
      "Episode 176000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained: 0. Times reached goal: 882.\n",
      "Episode 177000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 889.\n",
      "Episode 178000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 897.\n",
      "Episode 179000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 887.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 180000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained: 0. Times reached goal: 896.\n",
      "Episode 181000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 895.\n",
      "Episode 182000 finished after 0 timesteps with r=0.0. Running score: 0.91. Times trained: 0. Times reached goal: 891.\n",
      "Episode 183000 finished after 0 timesteps with r=0.0. Running score: 0.9. Times trained: 0. Times reached goal: 904.\n",
      "Episode 184000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 902.\n",
      "Episode 185000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 891.\n",
      "Episode 186000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 907.\n",
      "Episode 187000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 897.\n",
      "Episode 188000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 893.\n",
      "Episode 189000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 903.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 190000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 906.\n",
      "Episode 191000 finished after 0 timesteps with r=0.0. Running score: 0.89. Times trained: 0. Times reached goal: 908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 192000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 920.\n",
      "Episode 193000 finished after 0 timesteps with r=0.0. Running score: 0.89. Times trained: 0. Times reached goal: 891.\n",
      "Episode 194000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 909.\n",
      "Episode 195000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 902.\n",
      "Episode 196000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 916.\n",
      "Episode 197000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 902.\n",
      "Episode 198000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 922.\n",
      "Episode 199000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 915.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 200000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 905.\n",
      "Episode 201000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 911.\n",
      "Episode 202000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 922.\n",
      "Episode 203000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 936.\n",
      "Episode 204000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 919.\n",
      "Episode 205000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 908.\n",
      "Episode 206000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 913.\n",
      "Episode 207000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 936.\n",
      "Episode 208000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 914.\n",
      "Episode 209000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 909.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 210000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 906.\n",
      "Episode 211000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 929.\n",
      "Episode 212000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 907.\n",
      "Episode 213000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 922.\n",
      "Episode 214000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 913.\n",
      "Episode 215000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 918.\n",
      "Episode 216000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 925.\n",
      "Episode 217000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 913.\n",
      "Episode 218000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 920.\n",
      "Episode 219000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 939.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 220000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 940.\n",
      "Episode 221000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 932.\n",
      "Episode 222000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 935.\n",
      "Episode 223000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 934.\n",
      "Episode 224000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 935.\n",
      "Episode 225000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 927.\n",
      "Episode 226000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 930.\n",
      "Episode 227000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 944.\n",
      "Episode 228000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 936.\n",
      "Episode 229000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 937.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=0 , \n",
      "new state=12, done=True\n",
      "Episode 230000 finished after 0 timesteps with r=0.0. Running score: 0.94. Times trained: 0. Times reached goal: 949.\n",
      "Episode 231000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 934.\n",
      "Episode 232000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 944.\n",
      "Episode 233000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 940.\n",
      "Episode 234000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 943.\n",
      "Episode 235000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 937.\n",
      "Episode 236000 finished after 0 timesteps with r=0.0. Running score: 0.95. Times trained: 0. Times reached goal: 940.\n",
      "Episode 237000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 940.\n",
      "Episode 238000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 948.\n",
      "Episode 239000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 947.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 240000 finished after 0 timesteps with r=0.0. Running score: 0.91. Times trained: 0. Times reached goal: 923.\n",
      "Episode 241000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained: 0. Times reached goal: 953.\n",
      "Episode 242000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 958.\n",
      "Episode 243000 finished after 0 timesteps with r=0.0. Running score: 0.86. Times trained: 0. Times reached goal: 931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 244000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 946.\n",
      "Episode 245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 943.\n",
      "Episode 246000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 954.\n",
      "Episode 247000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 941.\n",
      "Episode 248000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 956.\n",
      "Episode 249000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 952.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 250000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 953.\n",
      "Episode 251000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 956.\n",
      "Episode 252000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 949.\n",
      "Episode 253000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 942.\n",
      "Episode 254000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 954.\n",
      "Episode 255000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 949.\n",
      "Episode 256000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 956.\n",
      "Episode 257000 finished after 0 timesteps with r=0.0. Running score: 0.98. Times trained: 0. Times reached goal: 955.\n",
      "Episode 258000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 954.\n",
      "Episode 259000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 965.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 260000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 960.\n",
      "Episode 261000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 963.\n",
      "Episode 262000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 967.\n",
      "Episode 263000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 959.\n",
      "Episode 264000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 958.\n",
      "Episode 265000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 951.\n",
      "Episode 266000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 963.\n",
      "Episode 267000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 964.\n",
      "Episode 268000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 959.\n",
      "Episode 269000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 965.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 270000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 971.\n",
      "Episode 271000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained: 0. Times reached goal: 962.\n",
      "Episode 272000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 965.\n",
      "Episode 273000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 963.\n",
      "Episode 274000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained: 0. Times reached goal: 959.\n",
      "Episode 275000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 964.\n",
      "Episode 276000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 969.\n",
      "Episode 277000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 963.\n",
      "Episode 278000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 971.\n",
      "Episode 279000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 969.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 280000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 965.\n",
      "Episode 281000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 979.\n",
      "Episode 282000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 964.\n",
      "Episode 283000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 965.\n",
      "Episode 284000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 973.\n",
      "Episode 285000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 963.\n",
      "Episode 286000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 970.\n",
      "Episode 287000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 975.\n",
      "Episode 288000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 975.\n",
      "Episode 289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 967.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 969.\n",
      "Episode 291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 292000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 966.\n",
      "Episode 293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 973.\n",
      "Episode 294000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 974.\n",
      "Episode 295000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 969.\n",
      "Episode 296000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 974.\n",
      "Episode 297000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 981.\n",
      "Episode 298000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 974.\n",
      "Episode 299000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 974.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 300000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 975.\n",
      "Episode 301000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 975.\n",
      "Episode 302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 974.\n",
      "Episode 303000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 975.\n",
      "Episode 304000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 974.\n",
      "Episode 305000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 980.\n",
      "Episode 306000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained: 0. Times reached goal: 973.\n",
      "Episode 307000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 972.\n",
      "Episode 308000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 978.\n",
      "Episode 309000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 979.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 310000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 979.\n",
      "Episode 311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 984.\n",
      "Episode 312000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 981.\n",
      "Episode 313000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 979.\n",
      "Episode 314000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 975.\n",
      "Episode 315000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 977.\n",
      "Episode 316000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 977.\n",
      "Episode 317000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 980.\n",
      "Episode 318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 984.\n",
      "Episode 319000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 983.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 320000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 984.\n",
      "Episode 321000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 979.\n",
      "Episode 322000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 981.\n",
      "Episode 323000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 979.\n",
      "Episode 324000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 978.\n",
      "Episode 325000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 978.\n",
      "Episode 326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 976.\n",
      "Episode 327000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 982.\n",
      "Episode 328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 985.\n",
      "Episode 329000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 977.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 330000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 979.\n",
      "Episode 331000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 978.\n",
      "Episode 332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 977.\n",
      "Episode 333000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 980.\n",
      "Episode 334000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 987.\n",
      "Episode 335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 988.\n",
      "Episode 336000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 983.\n",
      "Episode 337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 983.\n",
      "Episode 338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 980.\n",
      "Episode 339000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 983.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 340000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 985.\n",
      "Episode 341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 986.\n",
      "Episode 342000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 343000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 344000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 982.\n",
      "Episode 345000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 988.\n",
      "Episode 346000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 347000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 983.\n",
      "Episode 348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 349000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 982.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 350000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 989.\n",
      "Episode 351000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 352000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 980.\n",
      "Episode 353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 987.\n",
      "Episode 355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 989.\n",
      "Episode 356000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 987.\n",
      "Episode 357000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 980.\n",
      "Episode 359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 987.\n",
      "Episode 361000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 989.\n",
      "Episode 362000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 992.\n",
      "Episode 363000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 985.\n",
      "Episode 364000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 991.\n",
      "Episode 365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 366000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 988.\n",
      "Episode 367000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 992.\n",
      "Episode 368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 369000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 994.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 372000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 992.\n",
      "Episode 373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 374000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "Episode 376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 378000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 989.\n",
      "Episode 379000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 983.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "Episode 381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 382000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 988.\n",
      "Episode 384000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 989.\n",
      "Episode 385000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 990.\n",
      "Episode 386000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 986.\n",
      "Episode 387000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 390000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 990.\n",
      "Episode 391000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 990.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 394000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 988.\n",
      "Episode 395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 396000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 991.\n",
      "Episode 397000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 402000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 990.\n",
      "Episode 403000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 989.\n",
      "Episode 404000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 993.\n",
      "Episode 405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 409000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 989.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 413000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 417000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 995.\n",
      "Episode 418000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 992.\n",
      "Episode 419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "Episode 421000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 422000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 423000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 996.\n",
      "Episode 424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 430000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 432000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 990.\n",
      "Episode 433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 434000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 994.\n",
      "Episode 435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 436000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 437000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 992.\n",
      "Episode 438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 442000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 446000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 449000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 450000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 451000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 452000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 453000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 456000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 459000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 469000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 994.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 483000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 488000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 996.\n",
      "Episode 489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 495000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 496000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 499000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 502000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 503000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 505000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 518000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 524000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 525000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 540000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 543000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 544000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 547000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 558000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 996.\n",
      "Episode 559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 574000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 576000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 589000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 607000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 613000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 629000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 665000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 894000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 945000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 1990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 1999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 2990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 2999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 3990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 3999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 4990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 4999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 5990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 5999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 6990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 6999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 7990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 7999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 8990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 8999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9000000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9001000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9002000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9003000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9004000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9005000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9006000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9007000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9008000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9009000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9010000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9011000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9012000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9013000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9014000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9015000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9016000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9017000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9018000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9019000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9020000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9021000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9022000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9023000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9024000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9025000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9026000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9027000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9028000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9029000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9030000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9031000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9032000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9033000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9034000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9035000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9036000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9037000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9038000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9039000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9040000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9041000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9042000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9043000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9044000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9045000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9046000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9047000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9048000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9049000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9050000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9051000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9052000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9053000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9054000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9055000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9056000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9057000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9058000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9059000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9060000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9061000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9062000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9063000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9064000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9065000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9066000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9067000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9068000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9069000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9070000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9071000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9072000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9073000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9074000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9075000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9076000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9077000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9078000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9079000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9080000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9081000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9082000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9083000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9084000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9085000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9086000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9087000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9088000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9089000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9090000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9091000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9092000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9093000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9094000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9095000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9096000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9097000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9098000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9099000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9100000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9101000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9102000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9103000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9104000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9105000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9106000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9107000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9108000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9109000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9110000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9111000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9112000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9113000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9114000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9115000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9116000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9117000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9118000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9119000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9120000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9121000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9122000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9123000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9124000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9125000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9126000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9127000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9128000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9129000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9130000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9131000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9132000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9133000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9134000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9135000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9136000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9137000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9138000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9139000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9140000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9141000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9142000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9143000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9144000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9145000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9146000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9147000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9148000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9149000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9150000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9151000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9152000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9153000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9154000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9155000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9156000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9157000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9158000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9159000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9160000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9161000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9162000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9163000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9164000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9165000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9166000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9167000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9168000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9169000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9170000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9171000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9172000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9173000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9174000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9175000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9176000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9177000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9178000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9179000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9180000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9181000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9182000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9183000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9184000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9185000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9186000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9187000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9188000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9189000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9190000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9191000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9192000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9193000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9194000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9195000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9196000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9197000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9198000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9199000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9200000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9201000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9202000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9203000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9204000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9205000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9206000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9207000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9208000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9209000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9210000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9211000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9212000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9213000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9214000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9215000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9216000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9217000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9218000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9219000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9220000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9221000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9222000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9223000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9224000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9225000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9226000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9227000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9228000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9229000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9230000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9231000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9232000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9233000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9234000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9235000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9236000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9237000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9238000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9239000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9240000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9241000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9242000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9243000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9244000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9245000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9246000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9247000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9248000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9249000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9250000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9251000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9252000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9253000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9254000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9255000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9256000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9257000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9258000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9259000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9260000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9261000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9262000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9263000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9264000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9265000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9266000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9267000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9268000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9269000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9270000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9271000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9272000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9273000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9274000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9275000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9276000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9277000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9278000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9279000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9280000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9281000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9282000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9283000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9284000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9285000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9287000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9288000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9289000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9290000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9291000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9292000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9293000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9294000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9295000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9296000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9297000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9298000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9299000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9300000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9301000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9302000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9303000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9304000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9305000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9306000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9307000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9308000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9309000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9310000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9311000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9313000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9314000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9316000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9317000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9318000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9319000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9320000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9321000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9322000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9323000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9324000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9325000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9327000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9328000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9329000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9330000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9331000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9332000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9333000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9334000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9335000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9336000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9337000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9339000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9340000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9341000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9342000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9343000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9344000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9345000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9347000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9348000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9349000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9350000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9351000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9352000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9353000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9355000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9357000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9360000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9362000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9363000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9364000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9367000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9370000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9371000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9372000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9373000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9374000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9376000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9377000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9380000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9381000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9382000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9385000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9386000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9388000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9390000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9391000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9394000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9396000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9397000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9398000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9399000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9401000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9405000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9407000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9408000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9413000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9415000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9416000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9418000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9420000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9421000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9423000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9424000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9425000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9426000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9427000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9430000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9431000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9436000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9440000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9441000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9443000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9455000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9461000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9465000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9469000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9475000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9479000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9480000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9485000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9488000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9512000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9515000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9516000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9517000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9518000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9521000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9523000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9534000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9545000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9590000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9612000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9614000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9615000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9616000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9617000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9624000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9627000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9628000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9630000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9633000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9637000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9642000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9647000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9649000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9650000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9651000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9656000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9660000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9665000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9667000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9673000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9692000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9693000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9704000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9712000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9718000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9721000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9724000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9725000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9748000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9781000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9796000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9820000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9824000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9826000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9828000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9830000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9831000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9834000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9844000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9856000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9863000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9865000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9883000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9892000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9893000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9900000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9908000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9918000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9919000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9939000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9950000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9964000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9971000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9975000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9976000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9986000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 9990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 9999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "\n",
    "NUM_EPISODES = 10000000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.00\n",
    "EPS_DECAY = 500000\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "\n",
    "q_function = []\n",
    "for s in range(16):\n",
    "    action_list = []\n",
    "    for a in range(4):\n",
    "        action_list.append(0.0)\n",
    "    q_function.append(action_list)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "\n",
    "\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        q_values = np.array(q_function[observation])\n",
    "        if sample >= eps_threshold: \n",
    "            \n",
    "            #action = q_values.max(1)[1] # First 1 is the dimension, second 1 is the index (this is argmax)\n",
    "            action = q_values.argmax()\n",
    "            \n",
    "        else:\n",
    "            action = np.random.randint(0,3)\n",
    "        \n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation\n",
    "        if k%10000 == 0:\n",
    "            print(\"q_values \")\n",
    "            print(q_values)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action) \n",
    "        new_state = observation\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        #memory.push(old_state, action, new_state, reward, done)\n",
    "\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "        #if done and reward != 1.0:\n",
    "        #    reward = -1.0\n",
    "\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        q_value_new_state = np.array(q_function[new_state]).argmax()\n",
    "        q_function[old_state][action] = q_function[old_state][action] + \\\n",
    "        ALPHA *( reward + (q_function[new_state][q_value_new_state] * GAMMA) - q_function[old_state][action])\n",
    "        \n",
    "    \n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "  \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
